# Dataset Maker - Fix Summary

## Date: 2025-11-18

## Issues Fixed

### 1. ONNX Runtime CUDAExecutionProvider Not Available âœ“ FIXED
**Problem**: Only CPUExecutionProvider available despite GPU present
**Root Cause**: Both `onnxruntime` (CPU) and `onnxruntime-gpu` packages installed, CPU taking precedence
**Solution**:
- Removed conflicting `onnxruntime` package
- Ensured only `onnxruntime-gpu==1.20.1` is installed
- Updated pyproject.toml to explicitly specify onnxruntime-gpu version

**Verification**:
```bash
$ uv run python -c "import onnxruntime as ort; print(ort.get_available_providers())"
['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
```

### 2. WhisperX std::bad_alloc on Import âœ“ FIXED
**Problem**: `std::bad_alloc` error when importing WhisperX/pyannote
**Root Cause**: `torchcodec` wheel incompatibility with WSL2 + RTX 6000 Blackwell/RTX 4090
- pyannote-audio 4.0+ requires torchcodec >0.6.0
- torchcodec 0.8.0 has a bug causing immediate crash on import in WSL2 environment

**Solution**:
1. Downgraded `pyannote-audio` from 4.0.1 to 3.x (3.4.0)
   - pyannote 3.x does NOT require torchcodec
   - Added constraint: `pyannote-audio>=3.1.1,<4.0`
2. Removed torchcodec dependency entirely
3. Updated WhisperX to v3.4.3 (git tag)
4. Ensured `setup_cuda_env` is imported first in all entry points
5. Reverted all lazy import workarounds (no longer needed)

**Files Modified**:
- `pyproject.toml` - Updated dependency versions
- `emilia_pipeline.py` - Reverted to normal imports, fixed import references
- `Emilia/models/whisper_asr.py` - Reverted lazy imports
- `verify_installation.py` - Re-enabled direct pyannote testing
- `setup_cuda_env.py` - Added OpenBLAS/MKL threading config

**Verification**:
```bash
$ uv run python -c "import pyannote.audio; print(pyannote.audio.__version__)"
3.4.0

$ uv run python verify_installation.py
ðŸŽ‰ All checks passed! Your installation is ready for RTX 6000 Blackwell.

$ uv run python test_functionality.py
ðŸŽ‰ All functionality tests passed!
```

## Current Package Versions

- **PyTorch**: 2.8.0+cu128
- **CUDA**: 12.8
- **WhisperX**: v3.4.3 (git tag from m-bain/whisperX)
- **pyannote.audio**: 3.4.0 (3.x branch, no torchcodec required)
- **ONNX Runtime**: 1.20.1 (GPU version with CUDA and TensorRT providers)
- **Python**: 3.11.14

## System Configuration

**Hardware**:
- GPU 0: NVIDIA RTX PRO 6000 Blackwell (96GB VRAM)
- GPU 1: NVIDIA GeForce RTX 4090 (24GB VRAM)
- Running on WSL2 (Linux 6.6.87.2-microsoft-standard-WSL2)

**Environment**:
- `CUDA_VISIBLE_DEVICES=0` (Using RTX 6000 Blackwell by default)
- `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True,max_split_size_mb:256,garbage_collection_threshold:0.7`
- OpenBLAS/MKL threading limited to prevent conflicts with CUDA

## Import Order (Critical for WSL2)

All entry point files now import `setup_cuda_env` FIRST:

```python
# CRITICAL: Import setup_cuda_env FIRST to configure CUDA memory allocator
# This prevents std::bad_alloc errors when loading torch/pyannote on Linux/WSL
import setup_cuda_env

# Then other imports...
import torch
import whisperx
from pyannote.audio import Pipeline
```

**Files with proper import order**:
- `verify_installation.py`
- `transcriber.py`
- `emilia_pipeline.py`
- `quick_test.py`
- `system_health_checker.py`
- `test_functionality.py`

## Testing

### Comprehensive Functionality Tests
Created `test_functionality.py` which tests:
- âœ“ Basic imports of all critical modules
- âœ“ PyTorch CUDA operations
- âœ“ ONNX Runtime CUDA provider
- âœ“ WhisperX model loading (tiny.en)
- âœ“ Pyannote Audio pipeline import
- âœ“ Emilia pipeline module import
- âœ“ Transcriber module import
- âœ“ Whisper ASR module functionality

All tests pass successfully!

### Verification Script Results
```
Passed: 7/7

âœ“ PASS   - Python Version
âœ“ PASS   - PyTorch & CUDA
âœ“ PASS   - ONNX Runtime
âœ“ PASS   - WhisperX
âœ“ PASS   - Pyannote Audio
âœ“ PASS   - Other Dependencies
âœ“ PASS   - WSL Environment
```

## Batch Size Recommendations

For RTX 4090 (24GB VRAM):
- Emilia pipeline batch_size: 8-16
- WhisperX chunk_size: 15-20
- Transcriber batch_size: 12-16

For RTX 6000 Blackwell (96GB VRAM):
- Emilia pipeline batch_size: 24-32
- WhisperX chunk_size: 25-30
- Transcriber batch_size: 24-32

âš ï¸ Start with lower values and increase if memory allows

## Known Warnings (Non-Critical)

The following deprecation warnings appear but do NOT affect functionality:
```
UserWarning: torchaudio._backend.list_audio_backends has been deprecated...
```

These warnings are from pyannote.audio and speechbrain dependencies about future TorchAudio API changes. They can be safely ignored.

## Key Lessons Learned

1. **torchcodec is the culprit**: Not memory shortage, but wheel incompatibility
2. **pyannote 3.x is stable**: Doesn't require torchcodec, works perfectly on WSL2
3. **setup_cuda_env must be first**: Critical for preventing std::bad_alloc on WSL2
4. **Lazy imports weren't needed**: Once torchcodec removed, normal imports work fine
5. **Test systematically**: Created comprehensive test suite to verify all functionality

## References

- Codex analysis: torchcodec wheel bug with WSL2 + RTX 6000/4090 + current CUDA/PyTorch
- pyannote-audio 4.0+ changelog: Requires torchcodec >0.6.0 for video codec support
- WhisperX v3.4.3: Stable release with fixes for memory leaks

## Next Steps

1. âœ… All critical issues resolved
2. âœ… Comprehensive tests passing
3. âœ… Full verification passing
4. Ready for production use!

Optional improvements:
- Set HF_TOKEN environment variable for pyannote model downloads
- Fine-tune batch sizes for optimal performance on RTX 6000 Blackwell
- Monitor for future pyannote 4.x releases with torchcodec fixes for WSL2
